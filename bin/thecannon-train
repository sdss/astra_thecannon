#!/usr/bin/env python
# encoding: utf-8

from __future__ import absolute_import, division, print_function, unicode_literals

import argparse
import multiprocessing as mp
import numpy as np
import os

from astropy.table import join, unique
from collections import Counter
from astropy.table import Table
from warnings import warn
from tqdm import tqdm 

from astra import log
from astra.utils.data_models import parse_descriptors
from astra.tools.spectrum import Spectrum1D

from thecannon import (CannonModel, continuum, vectorizer)


def _get_most_probable_cross_match(paths, labels, ignore_duplicates=False):
    r"""
    Given a table of labels, and a set of input paths, identify the rows in the 
    labels table that most likely correspond to the input paths. The input paths 
    are parsed by the data models, and the data model keywords are used to
    match against the labels table.

    :param paths:
        A list of input paths of reduced data products.

    :param labels:
        A table of labels, which includes columns that match the same names that
        are expected from the keyword descriptors in the paths.
    """

    # Get descriptors for every path first.
    data_model, descriptors = parse_descriptors(paths[0])
    consider_label_names = set(descriptors.keys()).intersection(labels.dtype.names)

    if ignore_duplicates:
        N_before = len(labels)
        labels = unique(labels, keys=list(consider_label_names))
        N_duplicates = N_before - len(labels)
        warn(f"There were {N_duplicates} rows (on {consider_label_names}) that will be *ignored*!")

    dtypes = {ln: labels[ln].data.dtype for ln in consider_label_names}

    N_paths = len(paths)
    t = {ln: np.empty(N_paths, dtype=dtypes[ln]) for ln in consider_label_names}

    for i, path in enumerate(paths):
        data_model, descriptors = parse_descriptors(path)
        for label_name in consider_label_names:
            value = descriptors[label_name]

            # Ensure we match whitespace padding that astropy.table inflicts upon us.
            if dtypes[label_name].kind in "S":
                value = ("{{0: <{0}}}".format(dtypes[label_name].itemsize)).format(value)
            t[label_name][i] = value

    t["__input_path__"] = paths

    t2 = join(t, labels)

    # Check for unique entries.
    g = t2.group_by(list(consider_label_names))
    diffs = np.diff(g.groups.indices)
    if max(set(diffs)) > 1 and not ignore_duplicates:
        indices = np.where(diffs > 1)[0]
        raise ValueError(f"""
            Multiple matches for {len(indices)} input paths: could not uniquely 
            define them from the available path descriptors. Either (1) give a
            LABEL_PATH file without duplicates (in {consider_label_names}),
            restrict the training set input spectra to avoid duplicates,
            or live life *very dangerously* and use the --ignore-duplicates flag.
            Examples of duplicates include:
            {g.groups[indices]}""".strip())

    return t2



if __name__ == '__main__':

    parser = argparse.ArgumentParser(prog=os.path.basename(__file__),
                        description="Train a data-driven model of stellar spectra using The Cannon")
    # Required arguments by Astra.
    parser.add_argument("-v", "--verbose", dest="verbose", action="store_true",
                        help="verbose logging")
    parser.add_argument("input_path",
                        help="an ascii file containing the paths to reduced data products (spectra)"\
                             " to use for training the model")
    parser.add_argument("output_dir",
                        help="directory for analysis outputs")

    # Label arguments.
    parser.add_argument("label_path",
                        help="the path to a data table that contains the labels for all input paths")
    parser.add_argument("label_names", 
                        help="a comma-separated list of the label names to include in the model")
    parser.add_argument("--ignore-duplicates", action="store_true",
                        help="if the INPUT_PATHs cannot be uniquely identified as rows in the "\
                             "LABEL_PATH file, then ignore the duplicate rows (default: False)")

    # Data arguments.
    parser.add_argument("--spectrum-index", default=0,
                        help="the index of the spectrum to use in the training set spectra "
                             "(default: 0)")

    # Continuum arguments.
    parser.add_argument("--no-continuum", action="store_true",
                        help="do not fit the pseudo-continuum")
    parser.add_argument("--continuum-regions-path", dest="continuum_regions_path", default=None,
                        help="the path to a file describing regions of continuum pixels")
    parser.add_argument("--continuum-length-scale", dest="continuum_length_scale", default=1400,
                        help="the length scale (in pixels) for the sine and cosine functions used "\
                             "in determining the continuum (default: 1400)")
    parser.add_argument("--continuum-order", dest="continuum_order", default=3,
                        help="the number of sine and cosine functions to use to fit the continuum "\
                             "(default: 3)")

    # Model arguments.
    parser.add_argument("--order", dest="order", default=1,
                        help="the polynomial order of the model to use (default: 1; linear)")
    parser.add_argument("--regularization", dest="regularization", default=0,
                        help="the L1 regularization hyperparameter to use (default: 0)")

    # Operational arguments.
    parser.add_argument("-t", "--threads", dest="threads", default=1,
                        help="number of parallel threads to use")

    # TODO: Consider a --clobber argument and relevant checks before we start?
    
    # We *must* use parser.parse_known_args() here otherwise this tool will fail
    # when Astra provides the -i flag, which here is assumed by default because
    # we would never train The Cannon with one spectrum, because that would be
    # dumb.
    args, unknown = parser.parse_known_args()
    if unknown:
        warn(f"Ignoring unknown arguments: {unknown}")

    # Load in all the input paths.
    log.info("Checking input paths..")
    with open(args.input_path, "r") as fp:
        input_paths = list(map(str.strip, fp.readlines()))

    # Check for duplicates in input_paths.
    if len(set(input_paths)) < len(input_paths):
        counts = Counter(input_paths)
        duplicates = "\n".join([f"\t{k}: {v} entries" for k, v in counts.items() if v > 1])
        parser.error(f"there are duplicate paths in {args.input_path}:\n"
                     f"{duplicates}")

    # Load in all the labels.
    log.info(f"Reading labels from {args.label_path}")
    labels = Table.read(args.label_path)

    # Ensure we can match the labels to the appropriate input paths.
    if len(labels) == len(input_paths):
        warn(f"Assuming each row in INPUT_PATHs corresponds to the same row in LABELS_PATH! "
              "If this is not true then you're going to have a very bad time!")

    else:
        # For each input_path we have to infer the properties of the object
        # and match it to things that are available in the labels file.
        log.info(f"There are a different number of rows in {args.label_path} than input paths")
        log.info(f"Cross-matching rows in {args.label_path} to their most likely input path..")
        labels = _get_most_probable_cross_match(input_paths, labels, 
                                                ignore_duplicates=args.ignore_duplicates)
        input_paths = labels["__input_path__"]

    # Build the labels array.
    label_names = args.label_names.split(",")
    log.info(f"Label names to use: {label_names}")
    training_set_labels = np.array([labels[ln] for ln in label_names]).T

    for i, label_name in enumerate(label_names):
        minmax = [np.min(training_set_labels.T[i]), np.max(training_set_labels.T[i])]
        log.info(f"Minimum/maximum {label_name} in training set: {minmax}")

    # Find the first useful spectrum.
    for first_spectrum_idx, input_path in enumerate(input_paths):
        try:
            N_pixels = Spectrum1D.read(input_path).wavelength.size

        except IOError:
            continue

        else:
            break

    else:
        raise IOError("all input paths are corrupt or missing")

    N_paths = len(input_paths)
    log.info(f"There are {N_paths} spectra in the training set")
    log.info(f"There are {N_pixels} assumed per spectrum")
    log.info(f"Using spectrum index {args.spectrum_index} (zero-indexed) from --spectrum-index")

    wavelengths = np.nan * np.ones((N_paths, N_pixels), dtype=float)
    training_set_flux = np.ones_like(wavelengths)
    training_set_ivar = np.zeros_like(wavelengths)

    # Continuum normalize if necessary.
    continuum_kwds = dict()
    if not args.no_continuum:
        continuum_kwds.update(L=args.continuum_length_scale, order=args.continuum_order)
        if args.continuum_regions_path is not None:
            continuum_kwds.update(continuum_regions=np.loadtxt(args.continuum_regions_path))

        log.info(f"Pseudo-continuum normalization keywords: {continuum_kwds}")
        log.info(f"Loading training set spectra and pseudo-continuum normalizing..")

    else:
        log.info(f"No pseudo-continuum normalization will take place!")
        log.info(f"Loading training set spectra..")

    for i, input_path in enumerate(tqdm(input_paths)):

        if not os.path.exists(input_path):
            log.info(f"Path {input_path} does not exist. Skipping..")
            continue

        try:
            spectrum = Spectrum1D.read(input_path, verbosity=0)

        except OSError:
            log.info(f"Exception raised when trying to load {input_path}. Skipping..")
            continue

        # TODO: Write a general parallelised wrapper in Astra that makes use of tqdm
        wavelengths[i] = spectrum.wavelength.value
        
        # Continuum normalize if necessary
        if continuum_kwds:
            norm_flux, norm_ivar, cont, meta = continuum.normalize(
                spectrum.wavelength.value, 
                spectrum.flux.value[args.spectrum_index], 
                spectrum.uncertainty.array[args.spectrum_index], **continuum_kwds)

            # continuum.normalize will always return a 2D array
            training_set_flux[i] = norm_flux[0]
            training_set_ivar[i] = norm_ivar[0]

        else:
            training_set_flux[i] = spectrum.flux.value[args.spectrum_index]
            training_set_ivar[i] = spectrum.uncertainty.array[args.spectrum_index]

    log.info(f"Training set spectra loaded")
    log.info(f"Creating common dispersion mapping..")

    # Put everything on a common wavelength scale.
    min_wl = np.nanmax(wavelengths[:, 0])
    max_wl = np.nanmin(wavelengths[:, -1])

    # Check for uniform sampling in linear or logarithmic space.
    lin_space = np.std(np.diff(wavelengths[first_spectrum_idx]))
    log_space = np.std(np.diff(np.log10(wavelengths[first_spectrum_idx])))

    is_log_lambda_spaced = (lin_space > log_space)
    if is_log_lambda_spaced:
        log.info("Identified as uniform spacing in log-wavelength")
        delta = np.mean(np.diff(np.log10(wavelengths[first_spectrum_idx])))
        common_wavelengths = 10**np.arange(np.log10(min_wl), np.log10(max_wl) + delta, delta)

    else:
        log.info("Identified as uniform spacing in linear wavelength")
        delta = np.mean(np.diff(wavelengths[first_spectrum_idx]))
        common_wavelengths = np.arange(min_wl, max_wl + delta, delta)

    log.info(f"Common wavelength grid {min_wl:.1f} to {max_wl:.1f} ({common_wavelengths.size} pixels")
    assert common_wavelengths.size == training_set_flux.shape[1]

    log.info(f"Re-sampling training set spectra onto common wavelength grid..")
    for i, (wave_, flux_, ivar_) \
    in enumerate(tqdm(zip(wavelengths, training_set_flux, training_set_ivar), total=N_paths)):
        training_set_flux[i] = np.interp(common_wavelengths, wave_, flux_)
        training_set_ivar[i] = np.interp(common_wavelengths, wave_, ivar_)

    log.info(f"Correcting bad pixels..")
    bad = (~np.isfinite(training_set_flux)) + (~np.isfinite(training_set_ivar))
    training_set_flux[bad] = 1.0
    training_set_ivar[bad] = 0.0
    N_bad = np.sum(bad)
    f_bad = 100 * N_bad/training_set_flux.size 
    log.info(f"Corrected {N_bad} pixels ({f_bad:.2e}%)")

    log.info(f"Creating the model")

    model = CannonModel(training_set_labels, training_set_flux, training_set_ivar, 
                        vectorizer=vectorizer.PolynomialVectorizer(label_names, args.order),
                        dispersion=common_wavelengths, regularization=args.regularization)

    log.info(f"Training the model {model}")
    model.train()
    
    log.info(f"Setting s2 by Hogg heuristic")
    model._set_s2_by_hogg_heuristic()

    # TODO: Consider running Q-A one-to-one plots? Or other sensible things?

    # Prepare outputs.
    os.makedirs(args.output_dir, exist_ok=True)
    log.info(f"Created output directory {args.output_dir}")

    # Write the model to disk.
    output_path = os.path.join(args.output_dir, "Cannon.model")

    model.write(output_path, include_training_set_spectra=False, overwrite=True)

    log.info(f"Model ({model}) written to {output_path}")

