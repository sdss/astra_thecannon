#!/usr/bin/env python
# encoding: utf-8

from __future__ import absolute_import, division, print_function, unicode_literals

import argparse
import numpy as np
import os
from tqdm import tqdm
from warnings import warn

from astra import log
from astra.tools.parsers.common import component_parser
from astra.tools.spectrum import SpectrumList

from thecannon import continuum
from thecannon.model import CannonModel


if __name__ == '__main__':

    parser = component_parser(prog=os.path.basename(__file__),
                              description="The Cannon: A data-driven model for stellar spectra")

    # Required arguments by Astra.
    parser.add_argument("-v", "--verbose", dest="verbose", action="store_true",
                        help="verbose logging")
    parser.add_argument("-i", "--from-file", action="store_true", default=False,
                        help="specifies that the INPUT_PATH is a text file that contains a list of "
                             "input paths that are separated by new lines")
    parser.add_argument("input_path",
                        help="local path to a reduced data product, or a file that contains a list "
                             "of paths to reduced data products if the -i flag is used")
    parser.add_argument("output_dir",
                        help="directory for analysis outputs")

    # Model arguments.
    parser.add_argument("model_path",
                        help="the path to a pre-trained model")

    # Data arguments.
    parser.add_argument("--spectrum-index", default=0,
                        help="the index of the spectrum to use in the training set spectra "
                             "(default: 0)")


    # Continuum arguments.
    parser.add_argument("--no-continuum", action="store_true",
                        help="do not fit the pseudo-continuum")
    parser.add_argument("--continuum-regions-path", dest="continuum_regions_path", default=None,
                        help="the path to a file describing regions of continuum pixels")
    parser.add_argument("--continuum-length-scale", dest="continuum_length_scale", default=1400,
                        help="the length scale (in pixels) for the sine and cosine functions used "\
                             "in determining the continuum (default: 1400)")
    parser.add_argument("--continuum-order", dest="continuum_order", default=3,
                        help="the number of sine and cosine functions to use to fit the continuum "\
                             "(default: 3)")

    # Optimization arguments.
    parser.add_argument("--initialisations", default=1,
                        help="the number of initial points to optimize from (default: 1)")

    # Operational arguments.
    parser.add_argument("-t", "--threads", dest="threads", default=1,
                        help="number of parallel threads to use")

    # We *must* use parser.parse_known_args() here otherwise this tool will fail
    # when Astra provides the -i flag, which here is assumed by default because
    # we would never train The Cannon with one spectrum, because that would be
    # dumb.
    args, unknown = parser.parse_known_args()
    if unknown:
        warn(f"Ignoring unknown arguments: {unknown}")


    # Load the model.
    model = CannonModel.read(args.model_path)
    log.info(f"Loaded Cannon model from {args.model_path}: {model}")

    # Check continuum.
    continuum_kwds = dict()
    if not args.no_continuum:
        continuum_kwds.update(L=args.continuum_length_scale, order=args.continuum_order)
        if args.continuum_regions_path is not None:
            continuum_kwds.update(continuum_regions=np.loadtxt(args.continuum_regions_path))

        log.info(f"Pseudo-continuum normalization keywords: {continuum_kwds}")

    else:
        log.info(f"No pseudo-continuum normalization will take place!")

    log.info(f"We will access index {args.spectrum_index} from available spectra in each input path")

    # Prepare outputs.
    os.makedirs(args.output_dir, exist_ok=True)
    log.info(f"Created output directory {args.output_dir}")

    # The N and message keywords are just to suppress any progressbar from model.test()
    test_kwds = dict(initialisations=args.initialisations, N=-1, message=None)

    # TODO: spread over threads?

    # For each input_path we must do the following:
    N_paths = len(args.input_paths)
    for i, input_path in enumerate(tqdm(args.input_paths, total=N_paths)):

        # 1. Load the spectra.
        # For some data models this could truly be a list of spectra of different sources.
        # But for most it will be a Spectrum1D that may have multiple visits and spectra that have
        # been combined in various ways.
        try:
            spectra = SpectrumList.read(input_path)

        except OSError:
            log.info(f"Exception raised when loading path {input_path}")
            continue

        if len(spectra) > 1:
            # The spectra are of different objects. This is the case for the MaStar spectral library.
            # TODO: Not sure if we *should* handle this yet.
            raise NotImplementedError("MaNGA MaStar spectra cannot be processed by this component yet")

        # TODO: allow spectrum index to be able to test on *all* spectra
        spectrum = spectra[0]
        N_spectra, N_pixels = spectrum.flux.shape

        # 2. Re-sample onto the model dispersion.
        # Re-sample onto the model dispersion.
        # TODO: put this into the cannon utils because it will be needed for
        #       training
        flux = np.interp(model.dispersion,
                         spectrum.wavelength.value,
                         spectrum.flux.value[args.spectrum_index])
        ivar = np.interp(model.dispersion,
                         spectrum.wavelength.value,
                         spectrum.uncertainty.array[args.spectrum_index])

        # 3. Continuum-normalize if instructed.
        # TODO: refactor continuum normalization to deal with Spectrum1D objects.
        if continuum_kwds:
            norm_flux, norm_ivar, cont, meta = continuum.normalize(
                spectrum.wavelength.value, 
                spectrum.flux.value[args.spectrum_index], 
                spectrum.uncertainty.array[args.spectrum_index], **continuum_kwds)

        else:
            norm_flux, norm_ivar = (flux, ivar)

        # 4. Run the test step on the spectra.
        labels, cov, meta = model.test(norm_flux, norm_ivar, **test_kwds)

        # 5. Save the outputs.
        # TODO: We need a data model specification for this.
        log.info(f"Inferred labels: {labels}")
        log.info(f"Metadata: {meta}")
        log.error("Not saving output because no data model exists yet!")

        """
        basename, extension = os.path.splitext(os.path.basename(input_path))
        output_path = os.path.join(args.output_dir, f"{basename}.pkl")

        with open(output_path, "wb") as fp:
            pickle.dump((labels, cov, meta), fp)

        log.info(f"Wrote output to {output_path}")
        """

